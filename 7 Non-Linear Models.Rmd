---
title: "7 Non-Linear Models"
author: "By: Udit (based on ISLR)"
geometry: "left=1cm,right=1cm,top=1cm,bottom=2cm"
output: pdf_document
---

## Setup

```{r}
library(ISLR2)
library(splines)   # using splines
library(gam)       # using GAM models
library(akima)     # surface plots

attach(Wage)
dim(Wage)
```

## Polynomial Regression

Regression of Wage ~ Age, up to polynomial of power 4.
Polynomial vs. Spline - most important drawback of polynomial being non-locality. 
That is the fitted function at a given value x0 depends on data values far from that point.

```{r}
fit.poly <- lm(wage~poly(age,4), data=Wage)
summary(fit.poly)

#Raw polynomials .. poly(age,4, raw=TRUE) or:
fit.temp = lm(wage~age+I(age^2)+I(age^3)+I(age^4),data=Wage) #I - treat it as-is
summary(fit.temp)
```

Function **poly()** generates a basis of *orthogonal polynomials*, which is preferred.
With orthogonal polynomials we can separately test each coefficient. In this case 
power-4 coefficient is not significant. 

References:  
* [Visualizing orthogonal polynomials link](https://mathoverflow.net/questions/38864/visualizing-orthogonal-polynomials)  
* [Raw vs. Orthogonal link](https://stackoverflow.com/questions/19484053/what-does-the-r-function-poly-really-do)

```{r}
#Poly() produces an orthogonal set of basis functions. For example:
par(mfrow=c(1,3))
matplot(seq(-3,3,length.out=90),
        matrix(poly(seq(-3,3,length.out=90),3, raw=FALSE), ncol=3),
        lty = 1, pch = 1, type="l")

matplot(seq(-3,3,length.out=90),
        matrix(poly(seq(-3,3,length.out=90),3, raw=TRUE), ncol=3),
        lty = 1, pch = 1, type="l")

# However the fitted values would be consistent.
plot(fitted(fit.poly), fitted(fit.temp))
```

Plotting the fitted function.

```{r fig.width=7, fig.height=6}
agelims=range(age)
age.grid = seq(from =agelims[1], to=agelims[2])
preds = predict(fit.poly, newdata=list(age=age.grid), se=TRUE)
se.bands = cbind(preds$fit+2*preds$se, preds$fit-2*preds$se)

#Plotting
plot(age,wage, col="darkgrey")
lines(age.grid, preds$fit, lwd=2, col="blue")   #line-width
matlines(age.grid, se.bands, col="blue", lty=2)  #line-type
```
 
Using **anova()** and **F-test** to test for significance of different variables
in a series of nested models.

Fit models ranging from linear to a degree-5 polynomial and seek to determine the
simplest model which is sufficient to explain the relationship between wage and age. 

Upto **$Age^3$** appears to be significant.
```{r}
fit.1 <- lm(wage~age, data=Wage)
fit.2 <- lm(wage~poly(age,2), data=Wage)
fit.3 <- lm(wage~poly(age,3), data=Wage)
fit.4 <- lm(wage~poly(age,4), data=Wage)
fit.5 <- lm(wage~poly(age,5), data=Wage)
anova(fit.1, fit.2, fit.3, fit.4, fit.5)
```

Since polynomials are orthogonal, we could have simply used the  p-values from 
degree-5 fit to review the results. F-stat is equal to $t-stat^2$
```{r}
coef ( summary (fit.5))
(-11.9830341)^2
```
ANOVA method works whether or not we used orthogonal polynomials; it also works 
when we have other terms in the model as well.

**$Age^3$** is not-significant when other variables are included. 
```{r}
fit.a <- lm(wage~education, data=Wage)
fit.b <- lm(wage~education+age, data=Wage)
fit.c <- lm(wage~education+poly(age,2), data=Wage)
fit.d <- lm(wage~education+poly(age,3), data=Wage)
anova(fit.a, fit.b, fit.c, fit.d)
anova(fit.a)
```

## Polynomial - LOGISTIC regression

Change Wage output variable to 0/1, with 1 for >$250k earners.
In GLM due to the way it functions, some of the orthogonality of coefficients is 
lost, therefore to decide inclusion/exclusion of variable, we'll need to rely on 
F-test.

**Predict()** function also provides probabilities, using **type = "response"** option,
however that would make the standard-errors/ confidence internal non-sensical.

```{r}
fit.log <- glm(I(wage>250)~poly(age,4), data=Wage, family=binomial)
summary(fit.log)
plot(age,fitted(fit.log))

#Calculate Predicted values + Standard Error band for LOGIT!!
preds = predict(fit.log, list(age=age.grid), se=T)
se.bands = preds$fit + cbind(fit=0, lower=-2*preds$se, upper=+2*preds$se)

#Converting from Log-Odds/LOGIT to Probability
prob.bands = exp(se.bands)/(1+exp(se.bands))

#Plotting
matplot(age.grid,prob.bands, col="blue", lwd=c(2,1,1), lty=c(1,2,2), type="l", ylim=c(0,0.1))
points(jitter(age), I(wage>250)/10, pch="|", cex=0.5)

# fit.log.a <- glm(I(wage>250)~poly(age,2), data=Wage, family=binomial)
# fit.log.b <- glm(I(wage>250)~poly(age,3), data=Wage, family=binomial)
# anova(fit.log.a, fit.log.b)
```

## Step Functions

Using **cut()**. Breaks can be manually assigned using **breaks option**.

The age < 33.5 category is left out, so the intercept coefficient of 94 can be
interpreted as the average salary for those under 33.5 years of age, and the other 
coefficients are average additional salary for those other age groups.
```{r}
table(cut(age,4))
fit.step = lm(wage~cut(age,4), data=Wage)
coef(summary(fit.step))

fit.step1 = lm(wage~cut(age,c(17,25,40,65,82)), data=Wage)

plot(age, wage, pch=20, col="gray")
points(age,fitted(fit.step), pch=20, col="blue")
points(age,fitted(fit.step1), col="red")
```

## Splines - Fixed-knot Cubic Spline

**bs()** generates the B-spline basis matrix for a polynomial spline (cubic by default.)
**ns()** generates natural spline.
[More explanation here.](https://bmcmedresmethodol.biomedcentral.com/articles/10.1186/s12874-019-0666-3)

```{r}
# 3 knots will lead to 7 DFs (K+4) = 1 intercept + 6 basis functions
# We can either specify knots or DFs
agelims=range(age)
age.grid = seq(from =agelims[1], to=agelims[2])

fit.spline = lm(wage~bs(age, knots=c(25, 40, 60)),data=Wage)
summary(fit.spline)
pred.spline = predict(fit.spline, list(age=age.grid), se=TRUE)

fit.nspline = lm(wage~ns(age, knots=c(25, 40, 60)),data=Wage)
summary(fit.spline)
pred.nspline = predict(fit.nspline, list(age=age.grid), se=TRUE)

plot(age, wage, col="gray", pch=20)
lines(age.grid, pred.spline$fit , col="blue", lwd=2)
lines(age.grid, pred.nspline$fit, col="red", lwd=2)
abline(v=c(25, 40, 60), lty=2, col="grey")
matlines(age.grid, cbind(pred.spline$fit + 2*pred.spline$se,
                         pred.spline$fit - 2*pred.spline$se,
                         pred.nspline$fit + 2*pred.nspline$se,
                         pred.nspline$fit + 2*pred.nspline$se), lty="dashed", col=c("blue", "blue", "red", "red"))
legend("topright", legend=c("Cubic Spline", "Natural Cubic Spline"), col= c("blue", "red"), lty=1, lwd=2)
```
## Splines - Smoothing-splines

Smoothing-splines do not require knot selection, as **each point is a knot.**  
But a smoothing parameter $lambda$. Resulting spline is a natural cubic spline.

```{r}
# Controlling smoothing parameter by (i) Effective degrees of freedom
fit.sm.spline = smooth.spline(age, wage, df=16)
plot(age, wage, col="gray")
lines(fit.sm.spline, col="red", lwd=2)

# ... or (ii) cross-validation
fit.sm.spline = smooth.spline(age, wage, cv=TRUE)
lines(fit.sm.spline, col="purple", lwd=2)
fit.sm.spline
```

## LOESS - Local Regression

The larger the span, the smoother the fit.

```{r}
plot(age, wage, xlim=agelims, col="gray")
title("LOESS - Local Regression")

fit1 <- loess(wage~age, span=0.2, data=Wage)
fit2 <- loess(wage~age, span=0.5, data=Wage)
lines(age.grid, predict(fit1, data.frame(age=age.grid)), col="red", lwd=2)
lines(age.grid, predict(fit2, data.frame(age=age.grid)), col="blue", lwd=2)
legend("topright", legend=c("Span=0.2/20%", "Span=0.5/50%"), col=c("Red", "Blue"), lty=1, lwd=2, cex=0.8)

```

## GAM: Generalized Additive Models

Mixing more than one predictors. Use **s()** to specify a **Smoothing Spline** fit in 
a GAM Formula.

The generic **plot()** function recognizes that _gam.m3_ is an object of class Gam, 
and invokes the appropriate **plot.Gam()** method.

Compelling evidence that a GAM with a linear function of year is better vs. one 
that doesn't include year (p-value = 0.00014). However, there is no evidence that 
a non-linear function of year is needed (p-value = 0.349).

```{r}
# Gam with smoothing spline
gam.m3 = gam(wage~s(age,df=5)+s(year,df=4)+education, data=Wage)
par(mfrow=c(1,3))
plot(gam.m3,se=T)

# Should YEAR be linear or non-linear?
gam.m1 <- gam(wage ~ s(age,5) + education, data=Wage)
gam.m2 <- gam(wage ~ year + s(age,5) + education, data=Wage)
anova(gam.m1, gam.m2, gam.m3, test="F")
```
The “Anova for Parametric Effects” p-values clearly demonstrate that year, age, and 
education are all highly statistically significant, even when only assuming a linear 
relationship. Alternatively, the “Anova for Nonparametric Effects” p-values for
year and age correspond to a null hypothesis of a linear relationship versus the
alternative of a non-linear relationship. The large p-value for year reinforces the 
conclusion from the ANOVA test that a linear function is adequate for this term.
```{r}
summary(gam.m3)
```

### GAM: LOGIT with smoothing spline 
```{r}
gam.l1 = gam(I(wage>250)~s(age,df=4)+year+education, data=Wage, family=binomial)
plot(gam.l1, se=T)

#High SE for "<HS Grad" category?
table(education, I(wage>250))
gam.l1.sub = gam(I(wage>250)~s(age,df=4)+year+education, data=Wage, 
                 family=binomial,
                 subset=(education != "1. < HS Grad"))
plot(gam.l1.sub, se=T)

gam.l2.sub = gam(I(wage>250)~s(age,df=4)+s(year,df=4)+education, data=Wage, 
                 family=binomial,
                 subset=(education != "1. < HS Grad"))
plot(gam.l2.sub, se=T)

anova(gam.l1.sub, gam.l2.sub, test="Chisq") #no-need for adding non-linear terms for 'year'
```

### GAM: Using LOESS & Interaction
```{r}
gam.lo <- gam(wage~s(year, df=4) + lo(age, span=0.7) + education, data=Wage)
plot.Gam(gam.lo, se=TRUE, col="green")

gam.lo.i <- gam(wage~lo(year, age, span=0.5)+ education, data=Wage)
plot(gam.lo.i)
```

### Using GAM plotting functionality with **lm()** models.
```{r}
par(mfrow=c(1,3))
lm1 = lm(wage~ns(age,df=4)+ns(year,df=4)+education,data=Wage)
plot.Gam(lm1, se=T)
```
